\section{Performance considerations}
\label{sec:performance}

Performance is an integral aspect of any application and as such Roslyn doesn't escape from it either. It's not just about compiling an assembly as fast as possible: that's just one of the usecases in which Roslyn is used. Think about other scenarios such as getting quick intellisense\footnote{Intellisense: context-aware hints as you type} or fluent syntax highlighting of text as you scroll but also affects other performance aspects such as the memory impact.

In this section we will look at a handful of techniques the Roslyn team uses to optimize the platform. These are general approaches that often apply their ideas in several areas of the codebase, sometimes through a common resource. This should indicate that these are often optimizations at a high level rather than very specific single-use optimizations. 

\subsection{Concurrency}
\label{sec:concurrency}

\epigraph{A concurrent system is one where a computation can make progress without waiting for all other computations to completeâ€”where more than one computation can make progress at "the same time"}{\textit{Abraham Silberschatz \\ \footnotesize{Operating System Concepts 9th edition}}}

When you think of performance, concurrency is often one of the first aspects that come to mind. As such, it also takes a prominent place in Roslyn's architecture. We have already established that one of the characteristics of a syntax tree is its immutability -- the inability to make changes to it after it is constructed. This is done with concurrency in mind: if we create a new tree for every concurrent operation, we have the guarantee that changes from operation X does not affect the tree that is being manipulated by operation Y. There are several more areas of concurrency though:

\subsubsection{Source parsing}
\label{sec:concur-source-parsing}

Every file containing source code is parsed independently of any other files. The file is parsed sequentially (from top to bottom) and multiple files can be parsed at once.\parencite{Sadov2014}

\subsubsection{Symbol binding }
\label{sec:concur-symbol-binding}

When identifiers are bound to symbols (see Chapter \ref{sec:binding-phase}) there is no real need to do this sequentially: in the end you basically just lookup the symbol in a table according to an identifier. One remark we have to add here though is the fact that a type's base members should also be bound when that type is being bound.

Consider the following example:

\lstset{style=csharp, caption={Why type hierarchy matters for binding}}
\begin{lstlisting}
class BaseType
{
	public virtual void Method() { }
	public void BaseMethod() { }	
}

class SubType : BaseType
{
	public override void Method()
	{
		base.BaseMethod();
	}
}

class AnotherType
{
	public void Method() { }
}

class StartUp 
{
	public static void Main(string[] args)
	{
		BaseType a = new SubType();
		a.Method();
		AnotherType b = new AnotherType();
		b.Method();
	}
}
\end{lstlisting}

When we bind \verb|SubType| we have to bind \verb|BaseType| as well because, as is indicated, there might be a semantical dependency: we have to know, for example, whether that \verb|override| keyword is appropriate there. This is not a problem when there is no explicit base type involved: we know the base type is just \verb|System.Object| which is likely already bound and as such there are no other types we have to investigate. For this reason we can bind unrelated symbols in no specific order.

\noindent We can say there is "implicit partial ordering".\parencite{Sadov2014}

\subsubsection{Method body compilation }
\label{sec:concur-symbol-binding}

Method bodies are bound and emitted on a type-by-type basis and in lexicographical order (based on the alphabet). This order is important: when compiling identical code multiple types it should emit the same result every time. Sometimes compiling a method creates an additional structure such as a state machine (as is the case with async/await and iterator blocks). This idea of a 'deterministic build' is looked at more closely in Chapter \ref{sec:deterministic-builds}. 

Important to note is that method bodies are not emitted if there were any declaration errors. If that is the case, only binding is done for those aspects that can help in diagnosing the issue.\parencite{Sadov2014}


\subsection{Small nodes}
\label{sec:small-nodes}

Another performance-oriented aspect is the so-called "memory-footprint"\footnote{Memory footprint: The amount of memory software uses when running}. When an application is executed, it uses memory to temporarily store data that makes up the program's workflow. This memory is (unfortunately) not unlimited: every device only has a certain amount available. The more memory is used, the less memory becomes available for other tasks and the sooner you run out. C\# and VB.NET are managed languages which means the allocating and freeing of memory is done for the developer. The freeing of memory is done through the "Garbage Collector" (GC), a specialized service that will free unused memory locations when it deems it to be necessary. Discussing the GC is outside of the scope of this paper but it is important to realize that the more memory we use, the more the GC will have to step in and free up some of the unused memory locations so we can re-allocate these.\parencite{Todorov2013}

Knowing that, we can take a look at the next performance aspect. As we know, syntax nodes are some of the most elemental constructs in our AST\footnote{AST: Abstract Syntax Tree}. Considering the sheer amount of nodes that a large applications exists of, it would pay off to keep these as small as possible. Doing so would reduce the amount of memory allocated which in turn would reduce the amount of garbage collections. A garbage collection is a relatively expensive operation since you basically put a hold on all active threads (except for the GC thread) so the GC can do its work.\parencite{Botelho2014}

For this reason the Roslyn team decided to store certain information related to syntax nodes in a place that is not associated with that specific syntax node's object.\parencite{Sadov2014} "Meta-information" like diagnostic info and annotations are the two prime examples of this. In any given code base, most syntax nodes won't have any diagnostic information attached to them nor do they have annotations. This means that providing a field on every single syntax node object to store this information would be a waste of memory: in the end, a field with a \verb|null| value will still use some memory due to its pointer. For a 32-bit process this will be a 4-bit pointer while a 64-bit process will use an 8-bit pointer\footnote{https://msdn.microsoft.com/en-us/library/system.intptr.size(v=vs.110).aspx}. Multiply this by all the relevant syntax nodes and you reach a sizable amount of wasted memory space.

What Roslyn does instead is store the diagnostics inside the syntax node's associated syntax tree and when the diagnostic information is actually requested, it will look them up inside that tree.

\noindent This becomes clear when we take a look at the execution path in the code:

\lstset{style=csharp, caption={CSharpSyntaxNode.GetDiagnostics}}
\begin{lstlisting}
public new IEnumerable<Diagnostic> GetDiagnostics()
{
	return this.SyntaxTree.GetDiagnostics(this);
}
\end{lstlisting}

\noindent which passes it on to the following chain of calls inside the syntax tree:

\lstset{style=csharp, caption={CSharpSyntaxTree.GetDiagnostics}}
\begin{lstlisting}
GetDiagnostics(node.Green, node.Position);
EnumerateDiagnostics(greenNode, position);
new SyntaxTreeDiagnosticEnumerator(this, node, position);
\end{lstlisting}

\noindent We can see a new specific enumerator is created which will traverse the syntax tree's nodes and return any diagnostics it finds. That is not the end, however. Eventually we still have to look up the diagnostic information and we know it's not stored inside the syntax node object. We follow the execution path again and now it becomes clear:

\lstset{style=csharp, caption={SyntaxTreeDiagnosticEnumerator.MoveNext}}
\begin{lstlisting}
node.GetDiagnostics(); // node is a GreenNode
\end{lstlisting}

\lstset{style=csharp, caption={GreenNode.GetDiagnostics}}
\begin{lstlisting}
if (s_diagnosticsTable.TryGetValue(this, out diags))
\end{lstlisting}

\noindent and eventually we reach a statically defined look-up table where we connect given syntax nodes with their respective diagnostic information:

\lstset{style=csharp, caption={GreenNode.s\_diagnosticsTable}}
\begin{lstlisting}
private static readonly 
	ConditionalWeakTable<GreenNode, DiagnosticInfo[]> 
		s_diagnosticsTable =
			new ConditionalWeakTable<	GreenNode, 
																DiagnosticInfo[]>();
\end{lstlisting}

This approach comes with the caveat that when there \textit{are} diagnostics or annotations, retrieving them will be more expensive compared to just accessing a field. A trade-off had to be made here between permanent extra memory-usage and occassional extra look-up time and apparently the former was deemed more important.

\subsection{Object re-use}
\label{sec:object-reuse}

We've already highlighted the importance of memory impact in section \ref{sec:small-nodes} by making the nodes as small as possible. There is another approach we can take to this by making \textit{duplicate} nodes take absolutely no space: we simply re-use existing objects. In section \ref{sec:re-use-measurements} we will mimic the experiments done by Robin Sedlaczek\parencite{Sedlaczek2015} based on an explanation by Vladimir Sadov\parencite{Sadov2014}.

\subsubsection{Re-using nodes}
\label{sec:re-use-nodes}

The first aspect we will look at is re-using our green nodes. As we established in section \ref{sec:rg-trees-solution}, green nodes have "vague" information: instead of a specific location in the syntax tree they instead contain the width of themselves. This is good because if it would have a specific location, we would (almost) never be able to re-use it: two distinct nodes in a single tree will be located at a different position. However by using the width instead we \textit{can} consider two similar nodes as identical because we merely look at their width which is the same for both.

We can demonstrate this in the following example where we look at the node that represents \verb|2 + 2|.

\lstset{style=csharp, caption={Re-using nodes based on width vs position}}
\begin{lstlisting}
int x = 2 + 2; // Width: 5; Position: 9
int y = 2 + 2; // Width: 5; Position: 23
\end{lstlisting}

Following out of this and the fact that green nodes don't contain any other uniquely identifying information such as a parent, we can conclude that a single green node can represent multiple identical subtrees. In order to see how this is implemented we have to take a look at the \verb|SyntaxNodeCache|\footnote{https://github.com/dotnet/roslyn/blob/master/src/Compilers/CSharp/Portable/Syntax/InternalSyntax/SyntaxNodeCache.cs}.

There are three main conclusions that we can take away from this implementation:

\begin{itemize}

\item Limited cache size

Only a limited amount of nodes is stored in this cache. A trade-off has to be made between execution time and memory impact and the Roslyn team has decided to use a cache size of 65536 items (\verb|1 << 16|) as evidenced here:

\lstset{style=csharp, caption={Definition of the SyntaxNodeCache's size}}
\begin{lstlisting}
private const int CacheSizeBits = 16;
private const int CacheSize = 1 << CacheSizeBits;
private const int CacheMask = CacheSize - 1;
\end{lstlisting}

\item First In, First out

When a node is added, a hash key will be calculated. As is typical with hash implementations, this should strive to reach a uniform distribution across the data structure (an array in this case) for optimal performance. Based on this hash key and the mask, the to-be-cached node will be inserted at a certain location of the array. If an entry already exists at this location it will be overwritten, hence the FIFO (First In, First Out) principle.

\lstset{style=csharp, caption={Inserting a node using FIFO}}
\begin{lstlisting}
var idx = hash & CacheMask;
s_cache[idx] = new Entry(hash, node);
\end{lstlisting}

\item Up to 3 children

The last important aspect of this cache is the fact that every node can only have up to three children. If it has any more, the chance of a cache miss is too big\parencite{Sadov2014} so these are not allowed in the first place. This is enforced by making sure there are only overloads available for 1, 2 and 3 nodes.

\lstset{style=csharp, caption={Caching up to three children}}
\begin{lstlisting}
private static bool CanBeCached(GreenNode child1)
{
	return 	child1 == null || child1.IsCacheable;
}

private static bool CanBeCached(GreenNode child1, 
																GreenNode child2)
{
	return 	CanBeCached(child1) && 
					CanBeCached(child2);
}

private static bool CanBeCached(GreenNode child1, 
																GreenNode child2, 
																GreenNode child3)
{
	return 	CanBeCached(child1) && 
					CanBeCached(child2) && 
					CanBeCached(child3);
}
\end{lstlisting}

\end{itemize}





\subsubsection{Re-using tokens}
\label{sec:re-use-tokens}

\subsubsection{Measurements}
\label{sec:re-use-measurements}

\subsection{Weak references}
\label{sec:weak-references}

\subsection{Object pooling}
\label{sec:object-pooling}

\subsection{Specialized collections}
\label{sec:specialized-collections}

\subsection{LINQ}
\label{sec:linq}
















